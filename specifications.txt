TODO:
Avoid detect and avoid large files
    Worst case try to access site map

Add functionality for relative links


DONE: Question 4 still shows ics main domain
        shows duplicate domains as separate
        duplicates caused by https/http/ and www or no www
        total up number of subdomains

DONE: Fix http and https

DONE: Honor the politeness delay for each site
    - done in config.ini for us

TODO: Crawl all pages with high textual information content
    - decide what high textual information content means


DONE: Detect and avoid infinite traps
    - DONE: avoid previously visited Links
    - avoid similar pages (seen in lecture notes?) IGNORE THIS
    - regex for url patterns


DONE: Detect and avoid sets of similar pages with no information
    - pages with url patterns(?)


TODO: Detect and avoid dead URLs that return a 200 status but no data (click here to see what the different HTTP status codes mean (Links to an external site.))


TODO: Detect and avoid crawling very large files, especially if they have low information value
    - low information value
        - likely for pages that fall under a pattern to repeat information
    - age?
